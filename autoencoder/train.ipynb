{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import nmrglue as ng\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import wofz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from ipywidgets import interact, fixed\n",
    "from numpy import cos, sin, arccos, pi as π, tan, exp\n",
    "from lmfit import Model\n",
    "from lmfit.models import PseudoVoigtModel, VoigtModel, LinearModel, ExponentialModel, StepModel, LorentzianModel\n",
    "import sys\n",
    "import scipy as sc\n",
    "import scipy.special as spl \n",
    "import nmrglue as ng\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.constants import Boltzmann\n",
    "from numpy import power as pwr\n",
    "from scipy.stats import truncnorm\n",
    "import MDAnalysis as mda\n",
    "from MDAnalysis.tests.datafiles import PSF, DCD, GRO, XTC\n",
    "import nglview as nv\n",
    "from IPython.core.display import Image\n",
    "import io\n",
    "import pandas as pd\n",
    "import ipywidgets \n",
    "from MDAnalysis.analysis import diffusionmap, align\n",
    "from MDAnalysis.analysis.base import (AnalysisBase,\n",
    "                                      AnalysisFromFunction,\n",
    "                                      analysis_class)\n",
    "import mdtraj as md\n",
    "from nglview.datafiles import PDB, XTC\n",
    "import MDAnalysis.analysis.rms as rms\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from contact_map import ContactFrequency, ContactDifference, RollingContactFrequency, ContactTrajectory\n",
    "import itertools\n",
    "from MDAnalysis import *\n",
    "from MDAnalysis.lib.formats.libdcd import DCDFile\n",
    "from sklearn import mixture\n",
    "from matplotlib import colors\n",
    "# import gromacs as gmx\n",
    "from Bio.PDB import *\n",
    "import gc\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import statistics as st\n",
    "from MDAnalysis.analysis.rms import RMSF\n",
    "import MDAnalysis.transformations.boxdimensions as mda_box\n",
    "# import pyemma.coordinates as coor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# import pyemma\n",
    "# import pyemma.msm as msm\n",
    "# import pyemma.plots as mplt\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner.tuners as kt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Dense, Flatten, Reshape, Conv2DTranspose,\\\n",
    "PReLU\n",
    "from tensorflow.keras.layers import LeakyReLU, ELU\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
    "from sklearn.datasets import make_classification\n",
    "from keras.datasets import mnist\n",
    "import os\n",
    "import pickle\n",
    "from keras.initializers import Constant, VarianceScaling\n",
    "from MDAnalysis.analysis import contacts\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from dense_autoencoder import dense_autoencoder, gpu_id\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f65c4",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d3317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = md.load_xtc(\"../traj_file.xtc\", top=\"../protein.pdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba89f7",
   "metadata": {},
   "source": [
    "### Cα Contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_contact = md.compute_contacts(traj, contacts='all', scheme='ca')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a688fae",
   "metadata": {},
   "source": [
    "### Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cv = ca_contact\n",
    "count = 50\n",
    "\n",
    "#-----Determine the percentage of the entire dataset to be used for training-----#\n",
    "percentage_of_training_data = 90\n",
    "\n",
    "#-----Determine the percentage of the entire dataset to be used for testing-----#\n",
    "percentage_of_testing_data = 100 - (percentage_of_training_data)\n",
    "\n",
    "#-----Determining the size of the training data-----#\n",
    "no_of_training_data = int(count*int(0.01*percentage_of_training_data*input_cv.shape[0]/count))\n",
    "\n",
    "#-----Determining the size of the testing data-----#\n",
    "no_of_testing_data = input_cv.shape[0] - no_of_training_data\n",
    "\n",
    "#-----Determine the training indexes-----#\n",
    "training_indexes = np.linspace(0, input_cv.shape[0]-1, no_of_training_data).astype('int')\n",
    "\n",
    "#-----Determine the testing indexes-----#\n",
    "testing_indexes = np.array(list(set(np.arange(input_cv.shape[0]).tolist()).difference(training_indexes.tolist())))\n",
    "#-----Generate the training data-----#\n",
    "training_data = []\n",
    "\n",
    "for i in range(len(training_indexes)):\n",
    "    training_data.append(input_cv[training_indexes[i]])\n",
    "    \n",
    "training_data = np.array(training_data)\n",
    "\n",
    "#-----Generate the testing data-----#\n",
    "testing_data = []\n",
    "\n",
    "for i in range(len(testing_indexes)):\n",
    "    testing_data.append(input_cv[testing_indexes[i]])\n",
    "    \n",
    "testing_data = np.array(testing_data)\n",
    "\n",
    "#-----Verify the complete, training and testing dataset-----#\n",
    "\n",
    "print(\"Length of total dataset = \", input_cv.shape[0])\n",
    "print(\"Length of training dataset = \", len(training_data))\n",
    "print(\"Length of testing dataset = \", len(testing_data))\n",
    "print(\"Length of (training + testing) dataset = \", len(training_data)+len(testing_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423aea5",
   "metadata": {},
   "source": [
    "### Parameters to run the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ebe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to run the autoencoder\n",
    "\n",
    "# -----ENCODER-----#\n",
    "encoder_neurons=[64, 48, 32, 16, 8, 4, 2]\n",
    "encoder_activation=['elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu']\n",
    "encoder_wt_initialization=['he_normal', 'he_normal', 'he_normal', 'he_normal', 'he_normal', 'he_normal', \\\n",
    "                           'he_normal']\n",
    "\n",
    "# -----DECODER-----#\n",
    "decoder_neurons=[2, 4, 8, 16, 32, 48, 64]\n",
    "decoder_activation=['elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu']\n",
    "decoder_wt_initialization=['he_normal', 'he_normal', 'he_normal', 'he_normal', 'he_normal', 'he_normal', \\\n",
    "                           'he_normal']\n",
    "\n",
    "#-----PARAMETERS-----#\n",
    "optimizer=Adam\n",
    "loss=MeanSquaredError()\n",
    "learning_rate=1e-3\n",
    "batch_size=100\n",
    "epochs=300\n",
    "shuffle=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a379c9",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71506eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing data\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(training_data)\n",
    "x_test  = scaler.fit_transform(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec718a3b",
   "metadata": {},
   "source": [
    "### Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU to be used\n",
    "gpu=0\n",
    "gpu_id(gpu=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f789b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of dimensions\n",
    "input_shape = input_cv.shape[1]\n",
    "\n",
    "# -----LATENT-----#\n",
    "latent_neurons=2\n",
    "latent_activation='elu'\n",
    "latent_wt_initialization='he_normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a412af0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----SAVING FILENAME-----#\n",
    "save_dir_init=f\"protein_latent_{latent_neurons}_actv_elu\"\n",
    "save_dir_name=f\"{save_dir_init}_batch_{batch_size}_epochs_{epochs}\"\n",
    "\n",
    "ae = dense_autoencoder(input_shape=input_shape,\n",
    "                       encoder_neurons=encoder_neurons,\n",
    "                       encoder_activation=encoder_activation,\n",
    "                       encoder_wt_initialization=encoder_wt_initialization,\n",
    "                       latent_neurons=latent_neurons,\n",
    "                       latent_activation=latent_activation,\n",
    "                       latent_wt_initialization=latent_wt_initialization,\n",
    "                       decoder_neurons=decoder_neurons,\n",
    "                       decoder_activation=decoder_activation,\n",
    "                       decoder_wt_initialization=decoder_wt_initialization)\n",
    "ae.summary()\n",
    "ae.compile(optimizer=optimizer,\n",
    "           learning_rate=learning_rate,\n",
    "           loss=loss)\n",
    "\n",
    "ae.train(x_train=x_train,\n",
    "         x_test=x_test,\n",
    "         y_train=x_train,\n",
    "         y_test=x_test,\n",
    "         batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         shuffle=shuffle)\n",
    "\n",
    "ae.save(save_dir_name=save_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2615062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
